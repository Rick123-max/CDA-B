# MOD 12
- **EVENT CODE 4689- PROCESS END**
- **EVENT CODE 4674 specified user exercised their user right, as specified in the privileges field**
- **EVENT CODE 4662 - DCSycn Attack; in the subcategory Audit Directory Service Access, audits basic information about users performing operations within AD for events specified in an object’s System Access-Control List (SACL)**
- **EVENT CODE 4624 - Successful logon**
- **EVENT CODE 4663 - Access to an Object**
## OSINT Techniques

### Overview of OSINT
![12354486-84e4-4401-975d-81ad1349ef53](https://github.com/user-attachments/assets/c5ee17f4-a54e-4b26-9bc9-bef4492cebd4)

- **OSINT** is gathering publicly-available data for use as intelligence in a specific operation or investigative purpose.
- Not all data that is available on the internet is OSINT.
- That data becomes OSINT when it is retrieved or applied to a specific purpose.

#### OSINT Framework
<img width="1260" height="906" alt="b5a74228-64f9-433d-94aa-19e1e9633745" src="https://github.com/user-attachments/assets/dac0b10f-4584-4a91-993e-ed5bdadfa866" />

### Online and Local Search Tools
#### _IntelligenceX
- _IntelligenceX brings several search engines and tools together under one website in an organized manner to conduct OSINT queries.
- _IntelligenceX performs searches using specific search terms (or selectors) including:
  - Email addresses
  - Domains
  - Uniform Resource Locator (URL)
  - IP
  - Classless Inter-Domain Routing (CIDR)
  - Bitcoin addresses
  - InterPlanetary File System (IPFS) hashes
- Searches occur in multiple locations such as:
  - Darknet: A form of internet overlay networks that can only be accessed with special software, settings, or authorization; often employs a custom communication protocol
  - Document sharing platforms
  - WHOIS dataPublic data leaks

#### Shodan
- Shodan is a search engine, but not for websites. It was built to search for internet-connected devices such as webcams, voice controllers, home security systems, etc.
- It can be used to search for devices using **default/generic credentials**, for **indicators of files or directories unique to a specific device**, **database**, **VoIP** product, or **webcam**.

### Social Media
#### Gramho
- Gramho analyzes Instagram accounts to provide statistics and compare it to others users.
- The predictability algorithm provides feedback on the account’s popularity.
- It also allows exploration and monitoring of user account anonymously.
- This can be used to analyze Instagram accounts and even extract the original images or videos from the accounts.
- Screenshots or screen recordings may not capture the image with full fidelity.

#### Social Bearing
- Social Bearing is intended for brand and market research, but can be used to find links, images, videos, and even geolocation on certain topics.
- Any website or application trying to access a Twitter stream or endpoint must first authenticate with the new API.
- Application and user authentication are the two forms of Twitter API authentication.
  - Both methods of authentication necessitate the use of access keys, which Twitter uses to monitor endpoint use and combat spam.
- A Twitter login is required to complete some searches using Social Bearing.
- Twitter phased out access to their old Application Programming Interface (API), which has been replaced by a new API.
  - Any website or application trying to access a Twitter stream or endpoint must first authenticate with the new API.

### DNS Data
- DNS data provides multiple types of OSINT and various tools to help you obtain it.

#### DNSDumpster
- DNSDumpster is employed to find hosts related to a desired domain.
- For an attacker’s perspective, locating visible hosts is a vital part of security assessment.
- Being able to quickly discover, map, and visualize the domain's attack surface is beneficial for OSINT.
- This may be used to discover weak points in the attack surface. Weak points may include old servers or forgotten web applications/servers
- OSINT of this type is useful to CPT members to identify terrain in mission partner systems that are exposed to the internet in unknown or unforeseen ways. 

<img width="817" height="875" alt="d9b918bb-b1cb-4d04-b1ce-1464a698f588" src="https://github.com/user-attachments/assets/7079a269-5c7f-48e9-94f8-e5e92dd91fa4" />

### Recon-ng
- Recon-ng is a reconnaissance framework designed to provide a complex environment for conducting open-source, web-based reconnaissance rapidly.
- However, Recon-ng performs web-based recon naissance  exclusively, so it is not intended to compete with Metasploit.
- This tool installs locally on a computer and runs from there, so all queries/requests are coming from the computer rather than a search engine
- The Interesting File Finder checks multiple hosts for interesting files in predictable locations.
  - An interesting file is a file containing environmental data, metadata, or configuration information not normally linked from another web page, however, it is stored in a location that is publicly accessible

### theHarvester
- theHarvester is a powerful tool for gathering OSINT.
  - Specifically, it gathers information associated with provided **names**, **emails**, **domains**, **IPs**, and **URLs** from various public data sources like **Google** or **LinkedIn**.

### TinEye
- TinEye is a reverse image search.
  - It uses **pattern recognition**, **computer vision**, **machine learning**, and **neural networks** to search for any uploaded image.
- Reverse image searches are also commonly used to identify potential spearphishing attempts, particularly over social media.

### Google Hacking
#### Google Hacking Database (GHDB)
<img width="1036" height="796" alt="9830bca2-6456-4f1f-a6c1-f59ec501355c" src="https://github.com/user-attachments/assets/ff2c49cc-3160-4219-8da6-7796509a48a0" />

- GHDB is a repository of search filters used on Google (and other search engines) to search for interesting and/or sensitive information
- It is hosted on Offensive Security’s Exploit Database website
-_ Google dorking_ uses the Google search engine’s innate abilities to locate various file types and webpages using specific search strings.
  - A device that is connected to the internet is all that is required to execute these searches.

### Data Visualization
#### OSINT Combine
<img width="1279" height="828" alt="83299638-e80c-4718-bd84-f99c54fd98c1" src="https://github.com/user-attachments/assets/6158416e-3c61-47a8-b3e7-5d6250eebe3c" />

- OSINT Combine provides quick data visualization from local .csv files.
- The visualization is populated directly in the browser without needing third-party tools.
- As you load .csv files, it continuously adds the information to the network diagram.
- All data is managed from the client side; no information is processed through OSINT Combine servers.
- One example of using OSINT Combine is through domain name enumeration

## OPSEC 101

### Overview of OPSEC
- OPSEC is the process for mitigating vulnerabilities and safeguarding confidential, important, or classified data.
- OPSEC is a vital CPT process because if mistakes happen or there is a failure to observe the misuse of the information, there is the potential for seepage
- The ability to recognize, store, and comprehend essential elements of knowledge about one's environment is known as Situational Awareness (SA).
  - Simply put, SA entails being conscious of what is happening around you.
- To protect confidential, important, or classified data, CPT members must adhere to the following precautions:
  - Properly dispose of sensitive information (Operation Plans [OPLAN], memorandums, classified documents, etc.).
  - Do not share sensitive information to unauthorized personnel.
  - Avoid discussing sensitive information in public or unauthorized areas.
  - Do not send, post, and/or list information over the web.
  - Do not leave computers, tablets, and/or mobile phones unattended in public areas.

### OPSEC Terms
- Below are a number of terms and definitions commonly used when discussing OPSEC:
  - **Adversary**: An enemy who has the potential and motive to harm Department of Defense (DoD) activities or operations; also referred to as a threat.
  - **Countermeasure**: Employing equipment and/or tactics with the aim of reducing the operational effectiveness of an adversary's activities.
    - Anything that effectively negates or mitigates an adversary's ability to exploit vulnerabilities qualifies as a countermeasure.
  - **Critical information**: Specific information about friendly intentions, skills, or activities that adversaries need in order to prepare and function effectively in order to ensure failure or undesirable consequences for friendly mission accomplishment.
  - **Indicator**: Anything that draws attention to crucial details or provides an opponent with information regarding the environment or situation.
  - **OPSEC**: An empirical method that identifies preparation, processes, or operations to deny an adversary knowledge about objectives and capabilities that is usually unclassified.
    - Other security disciplines are not replaced by OPSEC; rather, they are supplemented by it.
  - **Risk**: A calculation of the insecurity of confidential information being compromised as a result of an adversary's exploitation.
  - **Risk analysis**: Specific vulnerabilities related to perceived or real-world security threat scenarios to evaluate the probability of sensitive information becoming compromised.
  - **Risk assessment**: A method of assessing the insecurity of information based on vulnerability to intelligence gathering and the severity of loss anticipated.
  - **Vulnerability**: An opportunity for an adversary to obtain sensitive information by exploiting a flaw. Anything that might give an adversary access to sensitive information.
- The OPSEC process includes the following five steps: (1) identification of critical information, (2) analysis of threats, (3) analysis of vulnerabilities, (4) assessment of risks, (5) application of appropriate countermeasures.

### HTTP vs HTTPS
- HTTPS is essentially HTTP using Transport Layer Security (TLS).
- The TLS protocol protects and encrypts information that is sent across the internet and combined with HTTP.
- A TLS connection ensures that the information sent between the server and web browser is encrypted.

<img width="1028" height="746" alt="bc13348b-8442-44e4-a5d1-a5e679cb518a" src="https://github.com/user-attachments/assets/a42f4daa-3da0-4292-ac81-23bfc3a791b6" />

### DNS over HTTPS
- DNS over HTTPS (DoH) serves as a solution to this concern.
- Widely adopted in 2020, DoH uses HTTPS essentially as an encrypted session layer for the DNS application data.
- In addition, there is DNS over TLS (DoT) that serves primarily the same function.
- Irrespective of which encryption solution is employed, encrypting DNS traffic limits the exposure of potentially mission-relevant information, and serves as an **OPSEC best practice**.

### Ensuring OPSEC with a VPN
- A VPN encapsulates and distributes network data over another network
- One use case for a VPN is to gain secure access to network services that should not be exposed to the public internet.
- An alternate use case is to encrypt traffic, transport it privately to another network, and then allow it to rejoin the public internet.
  - This serves to anonymize the traffic, or disassociate it from its point of origin.
  - By doing this, there is a reduced chance of mission-relevant data seeping publicly, and is an **OPSEC best practice**.
- For example, a CPT requires access to the internet, and has to rely on public Wireless Fidelity (WiFi).
- A VPN provides the extra layer of security and anonymity while allowing the CPT to fulfill its mission objectives. 

# MODULE 13

## The Traditional SIEM

### Siem Overview
<img width="2112" height="948" alt="b832dfc4-bc6d-43c6-b9ee-e9a27e8825db" src="https://github.com/user-attachments/assets/cd279529-833c-4c23-b59b-bed2a3bae86c" />

#### What is a SIEM?
- A **SIEM** is central to the modern Information Technology (IT) infrastructure
- It is a software solution that **aggregates and analyzes activity from many different resources**.
- A **SIEM** processes **data coming into and departing from networks and systems that are under an organization’s control**.
- Some SIEMs come with extensive functionality for data indexing, presentation, and alerts.
- However, the data ingested into a SIEM is only as useful as the accuracy of the data indices and filtering capabilities.
- The success of a SIEM is directly related to the administrator’s expertise with the accuracy of the data.
- A **SIEM** collects **data from hosts on the network** including: **switches**, **routers**, **servers**, Domain Controllers (**DC**), and **workstations**.
- SIEMs **store**, **normalize**, and **aggregate data analytics**, which allows analysts to **discover trends**, **detect threats**, and **enable organizations to investigate alerts**.

### Capabilities and Applications
- A SIEM has a range of capabilities that, when combined and integrated, offer **security monitoring and correlation** of multiple data sources
- The software allows security teams to gain insights on the attacker with threat rules derived from attacker Tactics, Techniques, and Procedures (TTP) and known Indicators of Compromise (IOC)
- The common capabilities and applications of a SIEM include:
  - Data Aggregation
    - SIEMs aggregate and consolidate data from across the enterprise’s network devices and endpoints.
    - Each node on the network continually processes large amounts of data. A SIEM is able to aggregate the data being processed.
  - Data Normalization
    - Aggregates and consolidates data from across enterprise's network devices
    - Aggregated data often has disparities and different fields.
    - Data normalization is the **process of structuring data commonly found in a relational database or SIEM**.
    - The data is structured in accordance with a series of normal forms.
    - The normal forms are filters designed to reduce data redundancy and improve data integrity.
    - The filters process the data by organizing it to avoid:
      - **Data Redundancy**: Unintentional occurrence of the same piece of data being held in two separate places
      - **Insertion Anomaly**: Inability to add data to the database or SIEM due to absence of other data
      - **Update Anomaly**: Data inconsistency that results from data redundancy
      - **Deletion Anomaly**: Unintended loss of data due to deletion of other data
  - Centralized Dashboard
    - SIEMs aggregate and consolidate data from across the enterprise’s network devices and endpoints into one centralized dashboard.
    - The dashboard is updated in realtime and allows an analyst to see a live snapshot of the enterprise.
  - Analysis and Security Event Correlation
    - SIEM enables analysts to access data and make decisions regarding potential signs of a data breach, threat, attack, or vulnerability.

### SIEM Security Monitoring
#### Use Cases
- An organization should leverage a SIEM’s capabilities to help in the following scenarios:
  - Anomaly Detection
    - Outliers and deviations from typical or expected behavior is an indicator of potentially Malicious Cyberspace Activity (MCA).
    - SIEM enables the ability to detect anomalous behaviors; these are behaviors out of the norm or unexpected.
    - Users take the information provided by the SIEM detection and further investigate the behavior.
    - For instance, the domain administrator on a network typically logs in at approximately 0800 and logs out at approximately 1600. The domain administrator follows this routine, give or take an hour, on a weekly basis.
    - However, recently the domain administrator's account was recorded as logging in and out at 0300. The SIEM collects the data regarding the anomaly and potentially alerts analysts.
  - Incident Response (IR)
    - SIEM does not provide IR alone; it enables analysts to see a real-time view of their network and make decisions regarding IR.
    - Analysts have a centralized view of machines and/or devices that may be affected. SIEM allows responders to query the network based on necessary requirements and make decisions quickly.
  - Insider Threat Preventions
    - Insider threats are a major vulnerability to organizations. Users who have access to the network need to be constantly monitored.
    - SIEM enables user monitoring; an analyst can create alerts and notifications based on anomalous or questionable activity that may occur on the network. Analysts are quickly able to identify the activity and make informed decisions.

### Data Science Overview
- Data science combines multiple fields including statistics, scientific methods, and data analysis to extract value from data
- Data scientists take the collected data, and apply analytics — such as mathematics and algorithms — to make sense of the data and enable informed decision making

  <img width="744" height="546" alt="92620081-bf55-4a06-8cd7-e9bc068ef31e" src="https://github.com/user-attachments/assets/5a4f197a-1281-41bb-83ca-8accf333bd1c" />

### Operationalizing Data Science
- Data science enables Security Operations (SecOps) to more effectively protect systems
- Data engineering enables SecOps by developing, constructing, testing, and maintaining databases and large-scale processing systems
- It is a primary responsibility of data engineers to recommend and aid in implementing methods to improve data reliability, efficiency, and quality. 
- Data science and data engineering allow for the following:
  - The creation, development, and maintenance of usable and scalable data repositories.
  - The ability to ingest data in a proper fashion, in order to maximize value and minimize expense (storage requirements, processing power, etc.).
  - The ability to ask advanced questions of a data set. For example, correlating multiple log sources together via advanced methods to produce a better picture of what is actually going on in an environment.
  - A better understanding of the inner workings of data platforms and data sets, so that results are properly and responsibly contextualized, communicated, and understood.

### SIEM Parts and Pieces
<img width="750" height="1200" alt="62de0e14-db1e-4716-97a5-17fb1e7b4629" src="https://github.com/user-attachments/assets/19245cf1-7e25-474f-a44f-f8ce9bd301c9" />

#### Data Source
- Data is collected at the source. The source can be from a variety of nodes on the network:
  - **Network devices**: Routers, switches, gateways
  - **Security devices**: Firewalls, proxies, Intrusion Detection Systems (IDS)
  - **Hosts**: Workstations and servers
- An agent can be deployed on the node; the agent is responsible for collecting data from the device it is installed on.
  - The agent packages and ships the data to the data management stage of the SIEM architecture
- Often, network devices such as routers and switches do not have agents, but leverage existing built-in functionality
- Network devices may use Simple Network Management Protocol (SNMP) and System Logging Protocol (Syslog) to send data to a centralized server or directly to the SIEM.

#### Data management
- At the Data Management phase of the SIEM architecture, data aggregation takes place; data is delivered from agents across the enterprise.
- The role of data management is to properly aggregate the data by type, location, etc.
- Data management is also responsible for data storage, where data is maintained and made available for access. 

#### Models and Analytics
- The Models and Analytics stage is where data science plays a key role.
- At this point of the architecture models, mathematics and schema are applied to the data; the result is data analytics.
- The analytics developed and applied at this stage drive analysis and decision-making by security analysts. 

#### User Interface
- At the User Interface (UI) phase of the SIEM architecture, data is displayed for the end-user via dashboards.
- The dashboards are populated by the data — both raw and manipulated — collected from the agents deployed across the enterprise.
- At this point, analysts view the models and analytics being executed — as well as create their own.
- The dashboards allow for customization and are able to be tailored to their exact purpose.
- The UI also allows analysts to query all the data collected

### Data Source Types and Manual Ingestion
#### Host-Based Security Systems (HBSS)
- What is it?
  - HBSS is a suite of Commercial-Off-The-Shelf (COTS) applications created by McAfee.
  - HBSS provides anti-virus detection rules. If a detection rule is triggered the user is alerted of the potentially harmful activity.
- How does it pertain to an investigation?
  - HBSS notifications of detection rule positive hits allow for starting points in IR.
  - When the anti-virus is triggered users are able to gather the information — what behavior triggered the notification, which node or network layer is affected, etc. — and start the investigation.
- Data Source and MITRE Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK)
  - Adversaries may attempt to get a listing of services running on remote hosts, including those that may be vulnerable to remote software exploitation.
  - Methods to acquire this information include port and vulnerability scans using tools that are brought onto a system.

#### Windows Events
- What is it?
  - Windows Operating Systems (OS) provide system process tracking through Windows event logs; viewable in Windows Event Viewer and exportable through .evtx files.
  - Windows event logs track unique occurrences on Windows OSs in the network.
  - These occurrences include:
    - Successful/failed logons
    - Account creation
    - Running a program
- How does it pertain to an investigation?
  - Windows event logs enable analysts to track events that occur on a Windows OS.
  - Analysts are able to view and connect occurrences of events and the time and date stamp associated with them.
- Data Source and MITRE ATT&CK
  - Adversaries may disable Windows event logging to limit data that can be leveraged for detections and audits.
    - The data is used by security tools and analysts to generate detections.
  - Adversaries may target system-wide logging or just that of a particular application.
    - By disabling Windows event logging, adversaries can operate while leaving less evidence of a compromise behind; this creates an anomaly and is potentially detectable.

#### System Monitor (Sysmon)
- What is it?
  - Windows Sysmon is a service installed on Windows OSs.
  - Sysmon monitors and logs activity to the Windows event log.
  - It provides detailed information about a wide variety of processes and connections including:
    - Process creations
    - Network connections
    - Changes to file creation time
- How does it pertain to an investigation?
  - Similarly to Windows events, Sysmon enables analysts to track specific events that occur on a Windows OS.
  - Analyst are able to view and connect occurrences of events and the time and date stamp associated with them.
- Data Source and MITRE ATT&CK
  - Adversaries may disable Sysmon logging to limit data that can be leveraged for detections and audits.
    - The data is used by security tools and analysts to generate detections.
  - Adversaries may target system-wide logging or just that of a particular application.
    - By disabling Sysmon event logging, adversaries can operate while leaving less evidence of a compromise behind.

#### Web Proxy
- What is it?
  - A web proxy is a tool that facilitates a user’s connection from their machine to a distant endpoint, normally external to their network and often specific to a particular service(s).
  - The proxy server interrupts the connection between the client system and the end server.
  - It acts as a go-between for the two endpoints.
  - Web proxies log all traffic via web proxy log files.
  - These files include:
    - Session duration
    - Hypertext Transfer Protocol (HTTP) status, method, and version
    - Bytes in/out
    - Uniform Resource Locator (URL) category, hostname, and path
    - Filename
    - User agent
- How does it pertain to an investigation?
  - Web proxy log files enable analysts to review connections made with external websites.
  - They can help determine what external system was reached and if data was transferred, and if so, how much was transferred.
- Data Source and MITRE ATT&CK
  - Adversaries may use a web proxy to act as an intermediary for network communications to a Command and Control (C2) server to avoid direct connections to their infrastructure.
  - Many tools exist that enable traffic redirection through proxies or port redirection.
  - Adversaries use these types of proxies to manage C2 communications, to provide resiliency in the face of connection loss, or to ride over existing trusted communications paths to avoid suspicion.

#### Web Server
- What is it?
  - A web server is an external-facing server on a domain.
  - Web servers are responsible for hosting software supporting a website.
  - The web server log files log traffic coming to the website the server is hosting on.
  - Every time a browser or user agent — Google included — requests any resource (pages, images, javascript file, etc.) from the web server, the activity is logged.
- How does it pertain to an investigation?
  - Web server log files enable analysts to view who has accessed their external-facing website.
  - Web server log files also enable analysts to see what might have been taken from the website.
- Data Source and MITRE ATT&CK
  - Adversaries may backdoor web servers with web shells to establish persistent access to systems.
  - A web shell is a web script placed on a breached web server to allow an adversary to use the web server as a gateway into a network.
  - A web shell provides a set of functions to execute or a Command Line Interface (CLI) on the system that hosts the web server.

#### Assured Compliance Assessment Solution (ACAS) — Extensible Markup Language (XML) Ingestion
- What is it?
  - ACAS is an enterprise vulnerability scanning capability for networks and components that are owned or operated by the Department of Defense (DoD).
- How does it pertain to an investigation?
  - ACAS scans are used to locate potentially compromised systems based on the vulnerabilities that may be present in the network.
- Data Source and MITRE ATT&CK
  - Adversaries may attempt to get a listing of services running on remote hosts, including those that may be vulnerable to remote software exploitation.
  - Methods to acquire this information include port and vulnerability scans using tools that are brought onto a system.

#### Electronic Mail (Email)
- What is it?
  - Email traffic that occurs through the email server on the network is logged through email log files.
  - Email log files include information such as:
    - Mail event (was the email delivered or received)
    - Email addresses
    - Remote host
    - Size of the message
- How does it pertain to an investigation?
  - Email log files enable analysts to see email communication across the network.
- Data Source and MITRE ATT&CK
  - Adversaries may target user email to collect sensitive information.
    - Emails may contain sensitive data, including trade secrets or Personally Identifiable Information (PII) that can prove valuable to adversaries.
    - Adversaries can collect or forward email from mail servers or clients.
  - Adversaries may also use access to a compromised server to further their phishing campaigns for additional internal exploitation or exploitation of partner organizations.

#### Firewall
- What is it?
  - A network firewall is the gateway into and out of the network, or portions of the network.
  - The firewall evaluates traffic crossing it, destined for it, or coming from it against a set of defined criteria (rules).
  - It determines what should happen with the traffic based on the definition of those rules.
  - Firewall log files include information such as:
    - Source Internet Protocol (IP) address
    - Source port
    - Destination IP address
    - Destination port
  - A host-based firewall is typically located on a specific host on the network.
    - A firewall protects the host from untrusted devices located on the same network.
    - Once configured, the firewall also protects the host from devices attempting to potentially attack the host via open ports and services.
- How does it pertain to an investigation?
  - Firewall log files enable analysts to view traffic that has attempted to or has successfully made it into and out of the network.
- Data Source and MITRE ATT&CK
  - Adversaries may disable or modify system firewalls in order to bypass controls limiting network usage.
  - Changes include disabling the entire mechanism as well as adding, deleting, or modifying particular rules.
    - This can be done numerous ways depending on the OS, including via CLI, editing Windows registry keys, and Windows control panel.
  - Adversaries might try to enumerate the rules of a firewall to determine what they can use to get through the firewall, and what capabilities they need to employ for C2 or data exfiltration once in.

#### Domain Name System (DNS)
- What is it?
  - DNS is frequently referred to as the phonebook of the internet.
  - When a request to access a domain is made, for example a user attempting to access ESPN.com, DNS takes that request and follows a set hierarchy to find the authoritative domain, and requests — normally — the IP address associated with the Fully Qualified Domain Name (FQDN).
  - Requests made via DNS are logged in DNS log files.
- How does it pertain to an investigation?
  - DNS log files enable analysts to view requests made by users to domains.
  - Analysts are able to view websites visited by users on their domain.
  - They are able to link the domains in the logs to known or suspicious domains.
- Data Source and MITRE ATT&CK
  - Adversaries may dynamically establish connections to C2 infrastructure to evade common detections and remediations.
  - DNS logs contain information that helps to identify such activity.

#### Packet Capture (PCAP)
- What is it?
  - PCAP is an Application Programming Interface (API).
  - The API captures live packet data from the network and stores it in PCAP files.
  - PCAP files capture packets traveling through multiple network layers.
  - PCAP files contain information such as network packets sent using Transmission Control Protocol (TCP)/IP and User Datagram Protocol (UDP).
- How does it pertain to an investigation?
  - PCAP files enable analysts to conduct packet analysis.
  - Packet analysis allows analysts to monitor bandwidth usage, identify Dynamic Host Configuration Protocol (DHCP) servers, and DNS resolution.
- Data Source and MITRE ATT&CK
  - Adversaries may sniff to collect the network traffic to capture information about an environment, including authentication material passed over the network and is included in PCAP files.
  - Network sniffing refers to using the network interface on a system to monitor or capture information sent over a wired or wireless connection.
    - An adversary may place a network interface into promiscuous mode to passively access data in transit over the network, or use Switched Port Analyzer (SPAN) ports to capture a larger amount of data.
  - Network sniffing may also reveal configuration details, such as running services, version numbers, and other network characteristics (e.g., IP addresses, hostnames, Virtual Local Area Network Identifiers [VLAN ID]) necessary for subsequent lateral movement and/or defense evasion activities.

### Deploying a Log Forwarder
#### Elastic Stack
- Elastic Stack was previously referred to as the ELK Stack. ELK refers to the three open-source projects:
  - **Elasticsearch**
    - **Elasticsearch** is an open-source, distributed, JavaScript Object Notation (JSON)-based search engine.
    - It stores, searches, and analyzes large volumes of data in near real-time and provides answers in milliseconds.
  - **Logstash**
    - **Logstash** ingests, transforms, and ships data regardless of format or complexity.
    - Data is often scattered or distributed across many systems in many formats.
    - **Logstash** supports a variety of inputs that synchronously pulls in events from a multitude of common sources.
    - To help users ingest data of different formats and schema, **Logstash** allows for **custom filters and pipelines**.
  - **Kibana**
    - **Kibana** is a free and open frontend application that sits on top of the Elastic Stack, providing search and data visualization capabilities for data indexed in Elasticsearch.
    - **Kibana** searches across all documents to create the visualizations and dashboards.
- In addition to the Elastic Stack is software called Beats.
  - **Beats** gathers data (such as Windows event logs).
  - **Beats** agents sit on servers with containers and centralizes the data in **Elasticsearch**.
  - **Beats** ships data that conforms with the Elastic Common Schema (ECS).
    - **ECS** is an open-source specification, developed with support from the Elastic user community.
    - **ECS** defines a common set of fields to be used when storing event data in Elasticsearch, such as logs and metrics.
    - **ECS** specifies the names and Elasticsearch datatypes for each field and provides descriptions for each.
    - If the data fits into the ECS, it can be directly imported into Elasticsearch, otherwise it has to be forwarded to Logstash.
- A critical distinction is that the Elastic Stack on its own is not a SIEM. How it is used and the analytics and alerting functions added to the Elastic Stack are what make it a SIEM. 

### Log Forwarder
- A log forwarder is a tool that runs on a network device or endpoint’s OS and automatically forwards event log records to a central collection server or SIEM.
- Log forwarders send events, based on the event source, event ID, users, computers, and keywords in the event. The events are delivered to the SIEM and allows analysts to take further action against the event.
- Log forwarders:
  - Automatically send specified event types to the SIEM according to its configuration
  - Export event data from network devices and endpoints
  - Filter events to forward by source, type ID, tool used, and specific keywords
  - Forward events to external systems to enable alerting, storing, and auditing activity on the network
- Winlogbeat is a data aggregation and forwarding agent that collects windows events and data. The information is packaged and delivered to Elastic Stack.
  - Located in `C:\ProgramData\Elastic\Beats\winlogbeat\winlogbeat.yml`
  - Pay attention to the `output.logstash` and `winlogbeat.event_logs` sections

### Manually Uploading Event Logs
- You must create a specific filtered log and save it as a **.evtx** file.
- After this, open powershell and run the `.\UploadScript.ps1` from the `C:\ProgramData\Elastic\Beats` directory
- Enter relevant information into the Script Promts and wait.
- 3. Enter the following information at each prompt:
     - Enter target directory path containing EVTX logs or folders grouping them by system (i.e., C:\Users\zburnham\EVTX-Logs): C:\Users\Trainee\Desktop\Host-Upload
     - Do you have nested folders labeled by system within this directory? (Default is NO) (y/n): n
     - Enter Client Name: hr-2
     - Enter Case # (i.e., Resolvn): 8888
     - Enter a searchable identifier or note for this evidence upload (i.e., coda-hr-2) Identifier: CDA


### Data Quality in a SIEM
#### Data Filtering
- SIEMs are responsible for the collection of large amounts of data.
- Each device on the network generates an event (data) when something happens.
- The data that is generated can be forwarded in realtime to the SIEM or is stored locally or in a database, and then sent to the SIEM at a later time.
- To make the data useable for analysis, SIEMs must standardize and save the data in uniform formats.
- A SIEM collects log data from the network it resides on, however, not all, not even the majority of it, is useful for SecOps.
- Only a small portion of the million logs that are collected by SIEM are useful to analysts.
- In order to get the data down to this 1%, the data is entered in a hypothetical funnel within the SIEM.
  - **The funnel applies algorithms, schema, and models that slowly whittle away the pertinent data**.
  - The data that remains is indexed, which allows for fast search and analysis. 

#### Data Normalization
- Data is collected from different devices across the network.
- Each device on the network produces unique event data, the types and frequency of events vary by device.
- In order to compare and analyze the data collected from the different devices across the network, data normalization must occur.
- Data normalization:
  - Is the process of structuring data in a standard process to reduce data redundancy and improve data integrity.
  - Consists of taking the log files and breaking them down into variables and fields that are similar and relevant.
  - Enables the analysis and comparison of activity across log types.

<img width="600" height="400" alt="6819d272-22e0-4fe2-8227-869a5bd7544b" src="https://github.com/user-attachments/assets/dda8091e-e25a-477f-8d18-6c21eb884343" />

#### Data Joining
- Since SIEMs receive data logs from every device on the network, it is common for the need to combine data sets.
- Data joining, or joins, is the process of combining data from different sources/sets into one.
- There are a few methods when joining data:

  <img width="600" height="400" alt="8a579ab9-90ab-43fd-a45e-085f09dbd0c9" src="https://github.com/user-attachments/assets/ec3ff856-7ed6-40b6-8309-9de885cd4c24" />


## Elastic Stack Overview

### Siem and Elastic Stack Architecture
#### SIEM and Elastic Stack Overview
- **SIEM**
  - A SIEM is central to the modern Information Technology (IT) infrastructure.
  - It is a software solution that aggregates and analyzes activity from many different resources.
  - A SIEM processes data coming into and departing from networks and systems that are under an organization’s control
  - The success of a SIEM is directly related to the administrator’s expertise with the accuracy of the data.
  - A SIEM collects data from hosts on the network including: switches, routers, servers, Domain Controllers (DC), and workstations.
  - A SIEM stores, normalizes, aggregates, and applies analytics to data, which allows analysts to discover trends, detect threats, and enable organizations to investigate alerts.

- **Elastic Stack**
  - Elastic Stack was previously referred to as the ELK Stack. ELK refers to the three open-source projects:
    - Elasticsearch
      - Elasticsearch is an open-source, distributed, JavaScript Object Notation (JSON)-based search engine.
      - It is often referred to as a Non-Structured Query Language (NoSQL) database, or document-oriented database as it does not require a user to specify a schema upfront.
      - It stores, searches, and analyzes large volumes of data in near real-time and provides answers in milliseconds.
    - Logstash
      - Logstash ingests, transforms, and ships data regardless of format or complexity.
      - Data is often scattered or distributed across many systems in many formats.
      - Logstash supports a variety of inputs that synchronously pulls in events from a multitude of common sources.
      - To help users ingest data of different formats and schema, Logstash allows for custom filters and pipelines.
      - The customization means that regardless of the log or data input structure — or the fields included — a user can write a filter to parse and index the data as desired.
    - Kibana
      - Kibana is a free and open frontend application that sits on top of the Elastic Stack, providing search and data visualization capabilities for data indexed in Elasticsearch.
      - Kibana queries the data residing in Elasticsearch, and searches across all documents to create visualizations and dashboards. 

<img width="1024" height="328" alt="83db71c0-bfa0-4092-bbf8-f8f4597eb973" src="https://github.com/user-attachments/assets/70633860-04b2-4cec-b9f6-1964eb79fc29" />

  - One important distinction is that Elastic Stack by itself is not a SIEM.
  - Rather, Elastic Stack is the platform that a SIEM can be built upon.
  - To make Elastic Stack a SIEM, analytics and reporting must be added.
  - Software and tools provided in Security Onion allow for analytics and reporting.

- **Beats**
  - Beats gathers data (such as Windows event logs).
  - Beats agents sit on servers with containers, and centralize the data in Elasticsearch.
  - Beats ships data that conforms with the Elastic Common Schema (ECS).
    - ECS is an open-source specification, developed with support from the Elastic user community.
    - ECS defines a common set of fields to be used when storing event data in Elasticsearch, such as logs and metrics.
    - ECS specifies field names and Elasticsearch datatypes for each field and provides descriptions.
    - If the data fits into the ECS, it can be directly imported into Elasticsearch, otherwise, it has to be forwarded to Logstash.

<img width="969" height="514" alt="1b2e96ef-eb0c-4464-aa44-2f0817fb70a4" src="https://github.com/user-attachments/assets/36917d0f-663d-47b6-ae63-6a50418977f3" />

- **SecOnion and Elastic Stack**
  - Security Onion is a free and open Linux distribution for threat hunting, enterprise security monitoring, and log management.
  - Security Onion offers full packet capture (both network-based and host-based Intrusion Detection Systems [IDS]), and includes powerful indexing, search, visualization, and analysis tools to make sense of those mountains of data.
  - The Elastic Stack has become a central component of recent versions of the Security Onion Linux distribution.

### Accessing SecOnion Logs
- Logs of information and data from endpoints on the network are accessed and analyzed.
- Security Onion not only includes data from endpoints on the network but also from other subsystems such as Suricata and Zeek

<img width="969" height="618" alt="4a00a83f-21bf-47e4-8080-49385496c1d7" src="https://github.com/user-attachments/assets/812b306d-8278-4eb9-9ef0-efbba7b8daad" />


### Operationalizing Elastic Stack
#### Detecting a DCSync Attack Using Elastic Stack
- DCSync is a credential dumping technique leading to the compromise of user credentials, and a prelude to the creation of a Golden Ticket — DCSync can compromise the Kerberos Service Account’s (**KRBTGT**) password.
  - The KRBTGT is a service account that encrypts authentication tokens for the DC.
- To create a Golden Ticket, an adversary must have access to the hash of the KRBTGT account.
  - The Golden Ticket utilizes Ticket Granting Tickets (TGT), which are files created by the Key Distribution Center (KDC) portion of the Kerberos authentication protocol.
  - The files are used to grant users access to network resources.
  - The Golden Ticket is a TGT, typically of an administrator’s account, where the expiration of the ticket is set beyond the normal expiration interval.
  - Once created, the only way to mitigate the Golden Ticket is to change the password of the KRBTGT account twice.
  - The password must be changed twice because doing so invalidates all TGTs that were previously issued. The KRBTGT password hash is used in the algorithm to create the TGT tickets.
- DCSync attacks occur when an adversary compromises a user’s **Replicating Directory Changes All** and **Replicating Directory Changes privileges** — administrators, domain administrators, enterprise administrators, and DC groups have these privileges by default.
  - If necessary, any user can be granted these specific privileges. Once obtained, an adversary replicates data (including credentials) from Active Directory (AD) using the Directory Replication Service (DRS) Remote Protocol.
- Using ID 4662, it is possible to see when a user exercises replicating directory changes, the goal of a successful DCSync attack. Correlating ID 4662 with other IDs, such as a successful logon (4624) or an attempt to access an object (4663), helps identify where and when a successful DCSync attack may have occurred.
- In addition to DCs, some applications (such as Azure AD Connect) have legitimate requirements for replication permissions. These applications are often targets for adversaries because of the requirements.

## Splunk Overview

### Introduction to Splunk
- There are three main components that make up a full Splunk deployment;
  - **forwarder**
    - A forwarder is responsible for collecting data from a source, and forwarding (i.e., sending) that data to another forwarder or to an indexer
    -  There are two types of forwarders:
      - universal forwarders
        - lightweight forwarder that only performs basic forwarding functionality.
        - It is not capable of acting as a full instance of Splunk; it simply collects raw data from a source and sends that raw data to a destination.
      - heavy forwarders.
        - provides additional functionality that a universal forwarder is not capable of.
        - These additional capabilities include
          - pre-processing data
          - routing that data to different destinations based on defined conditions.
        - Because a heavy forwarder can perform this extra processing, it commonly forwards data directly to indexers rather than to other forwarders.
        - A heavy forwarder can act as a full instance of Splunk.
  - **indexer**
    - handles data processing and storage via the indexing process.
      - In the indexing process, raw data is parsed and transformed into events.
      - These events are then stored in an index located on the indexer. Multiple indexes can exist on a single indexer.
    - In advanced Splunk deployments, multiple indexers may exist.
      - Usually, these indexers handle data coming from different sources.
      - Advanced deployments may choose to set up multiple indexers to act as a cluster.
        - Clustered indexers receive and replicate the same data across each indexer in the cluster.
        - This can increase performance of the Splunk deployment, and act as protection against data loss.
  - **search head**
    - A search head is responsible for parsing search queries into search jobs and distributing those search jobs to indexers.
    - Indexers process a search job against the indexes stored within that indexer, and return the results of the search job back to the search head.
    - The search head performs any necessary post-processing of the results before making the results available to the interface that queried the search head.
- These components can be combined together and deployed as a Splunk **instance**.

#### Splunk Deployment
<img width="723" height="371" alt="b48abfd7-48c9-4b53-8d40-6d24ca21d31a" src="https://github.com/user-attachments/assets/443c0854-415d-4139-99d8-69de6ad26f5d" />

#### Splunk Instancees
- A Splunk instance can be configured to act as one or more of the above components.
- For example, a single Splunk Enterprise instance running on a machine can act as:
  - **A heavy forwarder**: Collecting and pre-processing raw data from the local machine or a remote source
  - **An indexer**: Parsing raw data into events and storing those events in indexes
  - **A search head**: Tasking search jobs to the local indexer
- An instance of Splunk Enterprise also contains additional features such as the Splunk web interface.
- The interface allows searches to be run against the local search head, and the configuration of the components deployed within that instance.
  
<img width="612" height="336" alt="64720799-8b72-47b7-b656-3efd4996671f" src="https://github.com/user-attachments/assets/7c47b021-cd59-4d20-8d7c-9b36179a55af" />

- Splunk instances can be configured to act very differently from each other, allowing them to take on niche roles when required.
- These roles include:
  - **Cluster managers**: Manage clusters of indexes
  - **Deployment servers**: Act as centralized configuration managers for other Splunk instances
  - **License masters**: Manage Splunk licenses for multiple other Splunk instances

### Injesting Data
#### Exporting Logs
- Ingesting data into a Splunk instance first requires identifying and collecting the raw data of interest

### Searching Terminology
#### Search Head
- As discussed earlier, a Splunk search head is responsible for parsing search queries and distributing search jobs to indexers, which hold parsed data as events stored within individual indexes.
- The term search head may also be used to refer to the Search & Reporting application within a Splunk web interface, since the interface is so tightly coupled with the actual search head installed on a Splunk instance.

#### Index
- After an indexer is done parsing raw data into individual events, these events are stored inside an index.
- An index is the data repository for a Splunk instance — events located within an index are written to that instance’s disk.
- Additionally, any events stored in an index become searchable by a search head.
- Many different indexes may exist within a single Splunk deployment, and new ones can be created by modifying configuration options on the indexer.

#### Events
- Events contain event data, which consists of formatted fields that the indexer has extracted from the raw data, as well as additional metadata about the event, such as the host (the device that generated the event), source (where the event originated — for example, a data file), source type (described below), and other information.
- Metadata is stored in default fields within an event — default fields are shared between all events within an index, and are usually great places to start a search query.

#### Field
- A field is a searchable name/value pair within data.
- For example, a field named ComputerName may contain the machine name of the computer that a particular event occurred on.
- Fields are the building blocks of searches within Splunk.

#### Source Type
- When an indexer is processing an event, it tries to identify the data structure of the event.
  - This term is similar to the term schema — an event’s data structure describes the different fields that are located in an event, and contains information that a Splunk indexer uses to parse the event into those different fields.
- The name of the data structure used to parse an event is stored in the sourcetype default field for that event.
- Knowledge of the source types available within a Splunk deployment can be extremely useful for an analyst, because events with a specific source type should all have similar fields within them.
  - For example, events with a sourcetype of `WinEventLog:Security` are probably from Security-related Windows events, and should contain fields like ComputerName, and EventCode, among others.
  - Searching on an individual source type is a great way for an analyst to quickly narrow down the scope of their search.
-
- NOTE: Splunk has support for add-ons, which can extend Splunk to provide additional functionality. One of these add-ons, the Common Information Model, extends source type and schema information for a variety of different data-generation platforms — including security platforms — and uses this information to standardize field names between different data sources.
- For example, two different antivirus products might generate events that contain the file hash of a malicious file.
- These field names may not overlap due to a difference in the naming convention between the antivirus products, even though they both store the same hash. With the Common Information Model add-on, these fields can be collapsed into the same field name at search time, potentially adding tons of value for an analyst.




